"""
λ¨λΈ μ²΄ν¬ν¬μΈνΈ μ €μ¥/λ΅λ“ μ ν‹Έλ¦¬ν‹°
ν•™μµ μ¤‘ λ¨λΈ μƒνƒλ¥Ό μ €μ¥ν•κ³  λ³µμ›ν•λ” κΈ°λ¥ μ κ³µ
"""

import torch
from pathlib import Path
from datetime import datetime
from typing import Dict, Optional, Any
import json


class CheckpointManager:
    """
    λ¨λΈ μ²΄ν¬ν¬μΈνΈ κ΄€λ¦¬ ν΄λμ¤
    λΌμ΄λ“λ³„ μ²΄ν¬ν¬μΈνΈ, μµκ³  μ„±λ¥ λ¨λΈ, μµμ‹  λ¨λΈμ„ κ΄€λ¦¬
    """
    
    def __init__(
        self,
        checkpoint_dir: Path = Path("checkpoints"),
        experiment_name: Optional[str] = None,
        save_best: bool = True,
        save_latest: bool = True,
        save_rounds: bool = True
    ):
        """
        Args:
            checkpoint_dir: μ²΄ν¬ν¬μΈνΈ μ €μ¥ λ””λ ‰ν† λ¦¬
            experiment_name: μ‹¤ν— μ΄λ¦„ (Noneμ΄λ©΄ νƒ€μ„μ¤νƒ¬ν”„ μ‚¬μ©)
            save_best: μµκ³  μ„±λ¥ λ¨λΈ μ €μ¥ μ—¬λ¶€
            save_latest: μµμ‹  λ¨λΈ μ €μ¥ μ—¬λ¶€
            save_rounds: λΌμ΄λ“λ³„ μ²΄ν¬ν¬μΈνΈ μ €μ¥ μ—¬λ¶€
        """
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # μ‹¤ν— μ΄λ¦„ μƒμ„±
        if experiment_name is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            experiment_name = f"experiment_{timestamp}"
        
        self.experiment_name = experiment_name
        self.experiment_dir = self.checkpoint_dir / experiment_name
        self.experiment_dir.mkdir(parents=True, exist_ok=True)
        
        self.save_best = save_best
        self.save_latest = save_latest
        self.save_rounds = save_rounds
        
        # μµκ³  μ„±λ¥ μ¶”μ 
        self.best_accuracy = 0.0
        self.best_round = 0
        
        print(f"[μ²΄ν¬ν¬μΈνΈ] μ €μ¥ λ””λ ‰ν† λ¦¬: {self.experiment_dir}")
    
    def save_checkpoint(
        self,
        model: torch.nn.Module,
        round_num: int,
        metrics: Dict[str, Any],
        config: Optional[Dict[str, Any]] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        is_best: bool = False
    ) -> Path:
        """
        μ²΄ν¬ν¬μΈνΈ μ €μ¥
        
        Args:
            model: μ €μ¥ν•  λ¨λΈ
            round_num: λΌμ΄λ“ λ²νΈ
            metrics: μ„±λ¥ λ©”νΈλ¦­ (accuracy, loss λ“±)
            config: ν•μ΄νΌνλΌλ―Έν„° μ„¤μ • (μ„ νƒμ‚¬ν•­)
            optimizer: μµν‹°λ§μ΄μ € μƒνƒ (μ„ νƒμ‚¬ν•­)
            is_best: μµκ³  μ„±λ¥ λ¨λΈμΈμ§€ μ—¬λ¶€
            
        Returns:
            μ €μ¥λ μ²΄ν¬ν¬μΈνΈ νμΌ κ²½λ΅
        """
        checkpoint = {
            'round': round_num,
            'model_state_dict': model.state_dict(),
            'metrics': metrics,
            'config': config or {},
            'timestamp': datetime.now().isoformat(),
            'experiment_name': self.experiment_name
        }
        
        # μµν‹°λ§μ΄μ € μƒνƒ μ¶”κ°€ (μλ” κ²½μ°)
        if optimizer is not None:
            checkpoint['optimizer_state_dict'] = optimizer.state_dict()
        
        saved_paths = []
        
        # λΌμ΄λ“λ³„ μ²΄ν¬ν¬μΈνΈ μ €μ¥
        if self.save_rounds:
            round_path = self.experiment_dir / f"round_{round_num:03d}.pth"
            torch.save(checkpoint, round_path)
            saved_paths.append(round_path)
            print(f"[μ²΄ν¬ν¬μΈνΈ] λΌμ΄λ“ {round_num} μ €μ¥: {round_path.name}")
        
        # μµμ‹  λ¨λΈ μ €μ¥
        if self.save_latest:
            latest_path = self.experiment_dir / "latest_model.pth"
            torch.save(checkpoint, latest_path)
            saved_paths.append(latest_path)
            print(f"[μ²΄ν¬ν¬μΈνΈ] μµμ‹  λ¨λΈ μ €μ¥: {latest_path.name}")
        
        # μµκ³  μ„±λ¥ λ¨λΈ μ €μ¥
        if self.save_best and is_best:
            best_path = self.experiment_dir / "best_model.pth"
            torch.save(checkpoint, best_path)
            saved_paths.append(best_path)
            print(f"[μ²΄ν¬ν¬μΈνΈ] β… μµκ³  μ„±λ¥ λ¨λΈ μ €μ¥: {best_path.name} (μ •ν™•λ„: {metrics.get('accuracy', 0):.4f})")
        
        # λ©”νƒ€λ°μ΄ν„° JSON μ €μ¥
        metadata_path = self.experiment_dir / "checkpoint_metadata.json"
        self._save_metadata(metadata_path, checkpoint)
        
        return saved_paths[0] if saved_paths else None
    
    def _save_metadata(self, metadata_path: Path, checkpoint: Dict):
        """μ²΄ν¬ν¬μΈνΈ λ©”νƒ€λ°μ΄ν„°λ¥Ό JSONμΌλ΅ μ €μ¥"""
        metadata = {
            'round': checkpoint['round'],
            'timestamp': checkpoint['timestamp'],
            'metrics': checkpoint['metrics'],
            'config': checkpoint.get('config', {})
        }
        
        # κΈ°μ΅΄ λ©”νƒ€λ°μ΄ν„° λ΅λ“ (μλ” κ²½μ°)
        if metadata_path.exists():
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    existing_metadata = json.load(f)
                    # λΌμ΄λ“λ³„ λ©”νƒ€λ°μ΄ν„° λ¦¬μ¤νΈλ΅ μ €μ¥
                    if 'rounds' not in existing_metadata:
                        existing_metadata['rounds'] = []
                    existing_metadata['rounds'].append(metadata)
                    metadata = existing_metadata
            except Exception as e:
                print(f"[μ²΄ν¬ν¬μΈνΈ] β οΈ  λ©”νƒ€λ°μ΄ν„° λ΅λ“ μ‹¤ν¨: {e}")
                metadata = {'rounds': [metadata]}
        else:
            metadata = {'rounds': [metadata]}
        
        # λ©”νƒ€λ°μ΄ν„° μ €μ¥
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    def load_checkpoint(
        self,
        checkpoint_path: Path,
        model: torch.nn.Module,
        optimizer: Optional[torch.optim.Optimizer] = None,
        device: Optional[torch.device] = None
    ) -> Dict[str, Any]:
        """
        μ²΄ν¬ν¬μΈνΈ λ΅λ“
        
        Args:
            checkpoint_path: μ²΄ν¬ν¬μΈνΈ νμΌ κ²½λ΅
            model: κ°€μ¤‘μΉλ¥Ό λ΅λ“ν•  λ¨λΈ
            optimizer: μµν‹°λ§μ΄μ € (μ„ νƒμ‚¬ν•­)
            device: λ””λ°”μ΄μ¤ (μ„ νƒμ‚¬ν•­)
            
        Returns:
            μ²΄ν¬ν¬μΈνΈ λ”•μ…”λ„λ¦¬ (metrics, config λ“± ν¬ν•¨)
        """
        checkpoint_path = Path(checkpoint_path)
        
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"μ²΄ν¬ν¬μΈνΈ νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        # λ¨λΈ κ°€μ¤‘μΉ λ΅λ“
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # μµν‹°λ§μ΄μ € μƒνƒ λ΅λ“ (μλ” κ²½μ°)
        if optimizer is not None and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        print(f"[μ²΄ν¬ν¬μΈνΈ] β… μ²΄ν¬ν¬μΈνΈ λ΅λ“ μ™„λ£: {checkpoint_path.name}")
        print(f"  - λΌμ΄λ“: {checkpoint.get('round', 'N/A')}")
        print(f"  - μ •ν™•λ„: {checkpoint.get('metrics', {}).get('accuracy', 0):.4f}")
        
        return checkpoint
    
    def load_best(self, model: torch.nn.Module, optimizer: Optional[torch.optim.Optimizer] = None) -> Dict[str, Any]:
        """μµκ³  μ„±λ¥ λ¨λΈ λ΅λ“"""
        best_path = self.experiment_dir / "best_model.pth"
        return self.load_checkpoint(best_path, model, optimizer)
    
    def load_latest(self, model: torch.nn.Module, optimizer: Optional[torch.optim.Optimizer] = None) -> Dict[str, Any]:
        """μµμ‹  λ¨λΈ λ΅λ“"""
        latest_path = self.experiment_dir / "latest_model.pth"
        return self.load_checkpoint(latest_path, model, optimizer)
    
    def load_round(self, round_num: int, model: torch.nn.Module, optimizer: Optional[torch.optim.Optimizer] = None) -> Dict[str, Any]:
        """νΉμ • λΌμ΄λ“ μ²΄ν¬ν¬μΈνΈ λ΅λ“"""
        round_path = self.experiment_dir / f"round_{round_num:03d}.pth"
        return self.load_checkpoint(round_path, model, optimizer)
    
    def update_best(self, accuracy: float, round_num: int) -> bool:
        """
        μµκ³  μ„±λ¥ μ—…λ°μ΄νΈ
        
        Args:
            accuracy: ν„μ¬ μ •ν™•λ„
            round_num: λΌμ΄λ“ λ²νΈ
            
        Returns:
            μµκ³  μ„±λ¥μ΄ κ°±μ‹ λμ—λ”μ§€ μ—¬λ¶€
        """
        if accuracy > self.best_accuracy:
            old_best = self.best_accuracy
            self.best_accuracy = accuracy
            self.best_round = round_num
            print(f"[μ²΄ν¬ν¬μΈνΈ] π― μµκ³  μ„±λ¥ κ°±μ‹ : {old_best:.4f} β†’ {accuracy:.4f} (λΌμ΄λ“ {round_num})")
            return True
        return False
    
    def get_checkpoint_dir(self) -> Path:
        """μ²΄ν¬ν¬μΈνΈ λ””λ ‰ν† λ¦¬ κ²½λ΅ λ°ν™"""
        return self.experiment_dir


def create_checkpoint_manager(
    checkpoint_dir: Path = Path("checkpoints"),
    experiment_name: Optional[str] = None,
    **kwargs
) -> CheckpointManager:
    """
    μ²΄ν¬ν¬μΈνΈ λ§¤λ‹μ € μƒμ„± ν—¬νΌ ν•¨μ
    
    Args:
        checkpoint_dir: μ²΄ν¬ν¬μΈνΈ μ €μ¥ λ””λ ‰ν† λ¦¬
        experiment_name: μ‹¤ν— μ΄λ¦„
        **kwargs: μ¶”κ°€ μµμ…
        
    Returns:
        CheckpointManager μΈμ¤ν„΄μ¤
    """
    return CheckpointManager(
        checkpoint_dir=checkpoint_dir,
        experiment_name=experiment_name,
        **kwargs
    )

