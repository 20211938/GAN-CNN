"""
CLIP ëª¨ë¸ ê²°í•¨ ê²€ì¶œ ì„±ëŠ¥ í‰ê°€ ë° ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸
"""

import argparse
import json
import cv2
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import torch

from models.clip_defect_detector import CLIPDefectDetector
from utils.bbox_utils import extract_bboxes_from_json, normalize_defect_type, calculate_iou


def visualize_detection_results(
    image: np.ndarray,
    gt_bboxes: List[Dict],
    gt_types: List[str],
    detected_regions: List[Dict],
    defect_type_scores: Dict[str, float],
    save_path: Optional[Path] = None,
    show: bool = True
):
    """
    ê²€ì¶œ ê²°ê³¼ë¥¼ ì‹œê°í™”
    
    Args:
        image: ì›ë³¸ ì´ë¯¸ì§€
        gt_bboxes: Ground Truth ë°”ìš´ë”© ë°•ìŠ¤ ë¦¬ìŠ¤íŠ¸
        gt_types: Ground Truth ê²°í•¨ ìœ í˜• ë¦¬ìŠ¤íŠ¸
        detected_regions: ê²€ì¶œëœ ì´ìƒ ì˜ì—­ ë¦¬ìŠ¤íŠ¸
        defect_type_scores: ê²°í•¨ ìœ í˜•ë³„ ì ìˆ˜
        save_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ì €ì¥ ì•ˆ í•¨)
        show: í™”ë©´ì— í‘œì‹œ ì—¬ë¶€
    """
    fig, axes = plt.subplots(1, 2, figsize=(20, 10))
    
    # ì›ë³¸ ì´ë¯¸ì§€ì— GT í‘œì‹œ
    ax1 = axes[0]
    ax1.imshow(image)
    ax1.set_title('Ground Truth', fontsize=16, fontweight='bold')
    ax1.axis('off')
    
    # GT ë°•ìŠ¤ í‘œì‹œ
    for bbox, defect_type in zip(gt_bboxes, gt_types):
        x1, y1 = bbox['x1'], bbox['y1']
        w, h = bbox['x2'] - bbox['x1'], bbox['y2'] - bbox['y1']
        rect = Rectangle((x1, y1), w, h, linewidth=2, edgecolor='green', 
                       facecolor='none', label=f'GT: {defect_type}')
        ax1.add_patch(rect)
        ax1.text(x1, y1 - 5, defect_type, color='green', fontsize=10, 
                fontweight='bold', bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))
    
    # ê²€ì¶œ ê²°ê³¼ í‘œì‹œ
    ax2 = axes[1]
    ax2.imshow(image)
    ax2.set_title('CLIP Detection Results', fontsize=16, fontweight='bold')
    ax2.axis('off')
    
    # ê²€ì¶œëœ ë°•ìŠ¤ í‘œì‹œ
    for i, region in enumerate(detected_regions):
        x1, y1 = region['x1'], region['y1']
        w, h = region['x2'] - region['x1'], region['y2'] - region['y1']
        score = region.get('score', 0.0)
        rect = Rectangle((x1, y1), w, h, linewidth=2, edgecolor='red', 
                        facecolor='none', alpha=0.7)
        ax2.add_patch(rect)
        ax2.text(x1, y1 - 5, f'Score: {score:.3f}', color='red', fontsize=10,
                fontweight='bold', bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))
    
    # ê²°í•¨ ìœ í˜•ë³„ ì ìˆ˜ í‘œì‹œ
    score_text = "Defect Type Scores:\n"
    for defect_type, score in sorted(defect_type_scores.items(), key=lambda x: x[1], reverse=True):
        score_text += f"  {defect_type}: {score:.3f}\n"
    
    ax2.text(10, image.shape[0] - 20, score_text, color='black', fontsize=10,
            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),
            verticalalignment='bottom')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"  ğŸ’¾ ì €ì¥: {save_path}")
    
    if show:
        plt.show()
    else:
        plt.close()


def evaluate_clip_detector(
    data_dir: Path,
    clip_model: CLIPDefectDetector,
    output_dir: Optional[Path] = None,
    max_images: Optional[int] = None,
    visualize: bool = True,
    save_images: bool = True,
    iou_threshold: float = 0.5
) -> Dict:
    """
    CLIP ëª¨ë¸ì˜ ê²°í•¨ ê²€ì¶œ ì„±ëŠ¥ í‰ê°€
    
    Args:
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
        clip_model: CLIP ëª¨ë¸
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        max_images: í‰ê°€í•  ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜ (Noneì´ë©´ ì „ì²´)
        visualize: ì‹œê°í™” ì—¬ë¶€
        save_images: ì´ë¯¸ì§€ ì €ì¥ ì—¬ë¶€
        iou_threshold: IoU ì„ê³„ê°’
        
    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print(f"\n{'='*70}")
    print(f"CLIP ëª¨ë¸ ê²°í•¨ ê²€ì¶œ ì„±ëŠ¥ í‰ê°€")
    print(f"{'='*70}")
    print(f"  â”œâ”€ ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}")
    print(f"  â”œâ”€ ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜: {max_images if max_images else 'ì „ì²´'}")
    print(f"  â”œâ”€ IoU ì„ê³„ê°’: {iou_threshold}")
    print(f"  â””â”€ ì‹œê°í™”: {'í™œì„±í™”' if visualize else 'ë¹„í™œì„±í™”'}")
    print(f"{'='*70}\n")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    if output_dir:
        output_dir.mkdir(parents=True, exist_ok=True)
        if save_images:
            vis_dir = output_dir / "visualizations"
            vis_dir.mkdir(parents=True, exist_ok=True)
    
    # ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
    image_files = list(data_dir.glob("*.jpg"))
    if max_images:
        image_files = image_files[:max_images]
    
    print(f"[1ë‹¨ê³„] ì´ë¯¸ì§€ íŒŒì¼ ê²€ìƒ‰ ì™„ë£Œ: {len(image_files)}ê°œ\n")
    
    # í†µê³„ ë³€ìˆ˜
    total_images = 0
    images_with_gt = 0
    images_with_detections = 0
    
    # ê²€ì¶œ í†µê³„
    total_gt_boxes = 0
    total_detected_boxes = 0
    matched_pairs = []
    
    # ê²°í•¨ ìœ í˜•ë³„ í†µê³„
    defect_type_stats = defaultdict(lambda: {
        'gt_count': 0,
        'detected_count': 0,
        'matched_count': 0,
        'scores': []
    })
    
    # ì´ë¯¸ì§€ë³„ ê²°ê³¼
    image_results = []
    
    from tqdm import tqdm
    pbar = tqdm(image_files, desc="í‰ê°€ ì§„í–‰", unit="image", ncols=100)
    
    for img_path in pbar:
        json_path = img_path.with_suffix(".jpg.json")
        if not json_path.exists():
            continue
        
        total_images += 1
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = cv2.imread(str(img_path))
        if image is None:
            continue
        
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # Ground Truth ì¶”ì¶œ
        gt_bboxes, gt_types = extract_bboxes_from_json(json_path)
        
        if len(gt_bboxes) == 0:
            continue
        
        images_with_gt += 1
        total_gt_boxes += len(gt_bboxes)
        
        # ê²°í•¨ ìœ í˜• ìˆ˜ì§‘
        defect_types = list(set(gt_types))
        defect_types = [normalize_defect_type(dt) for dt in defect_types]
        
        # CLIPìœ¼ë¡œ ê²°í•¨ ê²€ì¶œ
        try:
            detection_result = clip_model.detect(
                image_rgb,
                defect_types=defect_types,
                bboxes=None  # ë°•ìŠ¤ ì—†ì´ ì „ì²´ ì´ë¯¸ì§€ ê¸°ë°˜ ê²€ì¶œ
            )
        except Exception as e:
            print(f"\nâš ï¸  ê²€ì¶œ ì‹¤íŒ¨ ({img_path.name}): {e}")
            continue
        
        detected_regions = detection_result.get('anomaly_regions', [])
        defect_type_scores = detection_result.get('defect_type_scores', {})
        
        if len(detected_regions) > 0:
            images_with_detections += 1
        
        total_detected_boxes += len(detected_regions)
        
        # GTì™€ ê²€ì¶œ ê²°ê³¼ ë§¤ì¹­ (IoU ê¸°ë°˜)
        matched = []
        unmatched_gt = list(range(len(gt_bboxes)))
        unmatched_det = list(range(len(detected_regions)))
        
        for i, gt_bbox in enumerate(gt_bboxes):
            gt_type = normalize_defect_type(gt_types[i])
            defect_type_stats[gt_type]['gt_count'] += 1
            
            best_iou = 0
            best_j = -1
            
            for j, det_region in enumerate(detected_regions):
                if j not in unmatched_det:
                    continue
                
                iou = calculate_iou(gt_bbox, det_region)
                if iou > best_iou:
                    best_iou = iou
                    best_j = j
            
            if best_iou >= iou_threshold and best_j != -1:
                matched.append({
                    'gt_idx': i,
                    'det_idx': best_j,
                    'iou': best_iou,
                    'gt_type': gt_type
                })
                unmatched_gt.remove(i)
                unmatched_det.remove(best_j)
                defect_type_stats[gt_type]['matched_count'] += 1
        
        # ë§¤ì¹­ë˜ì§€ ì•Šì€ ê²€ì¶œ ê²°ê³¼ë„ í†µê³„ì— ì¶”ê°€
        for j in unmatched_det:
            det_region = detected_regions[j]
            score = det_region.get('score', 0.0)
            # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ê²°í•¨ ìœ í˜• ì°¾ê¸°
            if defect_type_scores:
                best_type = max(defect_type_scores.items(), key=lambda x: x[1])[0]
                defect_type_stats[best_type]['detected_count'] += 1
                defect_type_stats[best_type]['scores'].append(score)
        
        matched_pairs.extend(matched)
        
        # ì´ë¯¸ì§€ë³„ ê²°ê³¼ ì €ì¥
        image_result = {
            'image_path': str(img_path),
            'gt_count': len(gt_bboxes),
            'detected_count': len(detected_regions),
            'matched_count': len(matched),
            'matched_pairs': matched,
            'unmatched_gt': unmatched_gt,
            'unmatched_det': unmatched_det
        }
        image_results.append(image_result)
        
        # ì‹œê°í™”
        if visualize and (save_images or len(image_results) <= 5):
            vis_path = None
            if save_images and output_dir:
                vis_path = vis_dir / f"{img_path.stem}_detection.png"
            
            visualize_detection_results(
                image_rgb,
                gt_bboxes,
                gt_types,
                detected_regions,
                defect_type_scores,
                save_path=vis_path,
                show=(len(image_results) <= 5)  # ì²˜ìŒ 5ê°œë§Œ í™”ë©´ í‘œì‹œ
            )
        
        pbar.set_postfix({
            'GT': total_gt_boxes,
            'ê²€ì¶œ': total_detected_boxes,
            'ë§¤ì¹­': len(matched_pairs)
        })
    
    pbar.close()
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
    print(f"\n{'='*70}")
    print(f"[2ë‹¨ê³„] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°")
    print(f"{'='*70}\n")
    
    # ì „ì²´ ë©”íŠ¸ë¦­
    precision = len(matched_pairs) / total_detected_boxes if total_detected_boxes > 0 else 0.0
    recall = len(matched_pairs) / total_gt_boxes if total_gt_boxes > 0 else 0.0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    # í‰ê·  IoU
    avg_iou = np.mean([m['iou'] for m in matched_pairs]) if matched_pairs else 0.0
    
    print(f"ğŸ“Š ì „ì²´ ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
    print(f"  â”œâ”€ í‰ê°€ ì´ë¯¸ì§€ ìˆ˜: {total_images}ê°œ")
    print(f"  â”œâ”€ GT ë°•ìŠ¤ ìˆ˜: {total_gt_boxes}ê°œ")
    print(f"  â”œâ”€ ê²€ì¶œ ë°•ìŠ¤ ìˆ˜: {total_detected_boxes}ê°œ")
    print(f"  â”œâ”€ ë§¤ì¹­ëœ ë°•ìŠ¤ ìˆ˜: {len(matched_pairs)}ê°œ")
    print(f"  â”œâ”€ Precision: {precision:.4f}")
    print(f"  â”œâ”€ Recall: {recall:.4f}")
    print(f"  â”œâ”€ F1-Score: {f1_score:.4f}")
    print(f"  â””â”€ í‰ê·  IoU: {avg_iou:.4f}")
    
    # ê²°í•¨ ìœ í˜•ë³„ ì„±ëŠ¥
    print(f"\nğŸ“Š ê²°í•¨ ìœ í˜•ë³„ ì„±ëŠ¥:")
    for defect_type, stats in sorted(defect_type_stats.items()):
        type_precision = stats['matched_count'] / stats['detected_count'] if stats['detected_count'] > 0 else 0.0
        type_recall = stats['matched_count'] / stats['gt_count'] if stats['gt_count'] > 0 else 0.0
        type_f1 = 2 * type_precision * type_recall / (type_precision + type_recall) if (type_precision + type_recall) > 0 else 0.0
        avg_score = np.mean(stats['scores']) if stats['scores'] else 0.0
        
        print(f"  â”œâ”€ {defect_type}:")
        print(f"  â”‚   â”œâ”€ GT: {stats['gt_count']}ê°œ")
        print(f"  â”‚   â”œâ”€ ê²€ì¶œ: {stats['detected_count']}ê°œ")
        print(f"  â”‚   â”œâ”€ ë§¤ì¹­: {stats['matched_count']}ê°œ")
        print(f"  â”‚   â”œâ”€ Precision: {type_precision:.4f}")
        print(f"  â”‚   â”œâ”€ Recall: {type_recall:.4f}")
        print(f"  â”‚   â”œâ”€ F1-Score: {type_f1:.4f}")
        print(f"  â”‚   â””â”€ í‰ê·  ì ìˆ˜: {avg_score:.4f}")
    
    # ê²°ê³¼ ì €ì¥
    results = {
        'total_images': total_images,
        'images_with_gt': images_with_gt,
        'images_with_detections': images_with_detections,
        'total_gt_boxes': total_gt_boxes,
        'total_detected_boxes': total_detected_boxes,
        'matched_pairs': len(matched_pairs),
        'precision': precision,
        'recall': recall,
        'f1_score': f1_score,
        'avg_iou': avg_iou,
        'defect_type_stats': {
            k: {
                'gt_count': v['gt_count'],
                'detected_count': v['detected_count'],
                'matched_count': v['matched_count'],
                'avg_score': float(np.mean(v['scores'])) if v['scores'] else 0.0
            }
            for k, v in defect_type_stats.items()
        },
        'image_results': image_results[:10]  # ì²˜ìŒ 10ê°œë§Œ ì €ì¥
    }
    
    if output_dir:
        results_path = output_dir / "evaluation_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {results_path}")
    
    print(f"\n{'='*70}\n")
    
    return results


def main():
    parser = argparse.ArgumentParser(description='CLIP ëª¨ë¸ ê²°í•¨ ê²€ì¶œ ì„±ëŠ¥ í‰ê°€')
    parser.add_argument(
        '--data-dir',
        type=Path,
        default=Path('data'),
        help='ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ (ê¸°ë³¸ê°’: data)'
    )
    parser.add_argument(
        '--output-dir',
        type=Path,
        default=Path('clip_evaluation_results'),
        help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: clip_evaluation_results)'
    )
    parser.add_argument(
        '--clip-model',
        type=str,
        default='ViT-B/32',
        help='CLIP ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: ViT-B/32)'
    )
    parser.add_argument(
        '--max-images',
        type=int,
        default=None,
        help='í‰ê°€í•  ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜ (ê¸°ë³¸ê°’: ì „ì²´)'
    )
    parser.add_argument(
        '--iou-threshold',
        type=float,
        default=0.5,
        help='IoU ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.5)'
    )
    parser.add_argument(
        '--no-visualize',
        action='store_true',
        help='ì‹œê°í™” ë¹„í™œì„±í™”'
    )
    parser.add_argument(
        '--no-save-images',
        action='store_true',
        help='ì´ë¯¸ì§€ ì €ì¥ ë¹„í™œì„±í™”'
    )
    parser.add_argument(
        '--device',
        type=str,
        default=None,
        help='ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (ê¸°ë³¸ê°’: ìë™ ê°ì§€)'
    )
    
    args = parser.parse_args()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device is None:
        args.device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    print(f"\n{'='*70}")
    print(f"CLIP ëª¨ë¸ ê²°í•¨ ê²€ì¶œ í‰ê°€ ë„êµ¬")
    print(f"{'='*70}")
    print(f"  â”œâ”€ ë°ì´í„° ë””ë ‰í† ë¦¬: {args.data_dir}")
    print(f"  â”œâ”€ ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
    print(f"  â”œâ”€ CLIP ëª¨ë¸: {args.clip_model}")
    print(f"  â”œâ”€ ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜: {args.max_images if args.max_images else 'ì „ì²´'}")
    print(f"  â”œâ”€ IoU ì„ê³„ê°’: {args.iou_threshold}")
    print(f"  â”œâ”€ ì‹œê°í™”: {'ë¹„í™œì„±í™”' if args.no_visualize else 'í™œì„±í™”'}")
    print(f"  â”œâ”€ ì´ë¯¸ì§€ ì €ì¥: {'ë¹„í™œì„±í™”' if args.no_save_images else 'í™œì„±í™”'}")
    print(f"  â””â”€ ë””ë°”ì´ìŠ¤: {args.device}")
    print(f"{'='*70}\n")
    
    # CLIP ëª¨ë¸ ì´ˆê¸°í™”
    print("[CLIP ëª¨ë¸ ì´ˆê¸°í™”]")
    try:
        device = torch.device(args.device)
        clip_model = CLIPDefectDetector(
            model_name=args.clip_model,
            device=device
        )
        print("[CLIP ëª¨ë¸ ì´ˆê¸°í™”] âœ… ì™„ë£Œ\n")
    except Exception as e:
        print(f"[CLIP ëª¨ë¸ ì´ˆê¸°í™”] âŒ ì‹¤íŒ¨: {e}")
        print("\nğŸ’¡ CLIP ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("   pip install git+https://github.com/openai/CLIP.git")
        return
    
    # í‰ê°€ ì‹¤í–‰
    results = evaluate_clip_detector(
        data_dir=args.data_dir,
        clip_model=clip_model,
        output_dir=args.output_dir,
        max_images=args.max_images,
        visualize=not args.no_visualize,
        save_images=not args.no_save_images,
        iou_threshold=args.iou_threshold
    )
    
    print("âœ… í‰ê°€ ì™„ë£Œ!")


if __name__ == '__main__':
    main()

